[serif,xcolor=pdftex,dvipsnames,table,hyperref=bookmarks=false]beamer



Support Vector Machines, Basis Expansion and Kernels







[t]Overview


12pt
To date we've seen one example of a discriminative linear classifier.

Today we'll introduce a second example, support vector machines.

We'll then address the question of how to increase the capacity of 
linear classifiers so they can produce non-linear classification boundaries.









[t]Support Vector Machines


8pt
A binary support vector machine is a discriminative classifier that takes labels in the set $\-1,1\$.

The decision function has the form:
$$f_SVM() = (^T+b )$$

It's easy to show that the decision boundary for logistic regression can be written in exactly the same way.

 If logistic regression and SVMs have the same form for their decision boundaries, how do the differ?




[t]Logistic Loss


8pt
In the case of logistic regression with $_2$ regularization, we select the model parameters by maximizing the function:

$$C_i=1^n P(Y=y_i|=_i) - ||||_2^2 $$

Under the assumption that the labels take the values $\-1,1\$, it can be shown that
this is equivalent to minimizing the function:

$$C_i=1^n (1+(-y_i g())) + ||||_2^2$$

where $L_log(y_i,g(_i)) = (1+(-y_i g()))$ is the  and 
$g() = ^T+b$.





[t]Hinge Loss


8pt
In the case of SVMs with $_2$ regularization, we select the model parameters by minimizing the function:


C_i=1^n (0,1-y_i g(_i)) +  ||||_2^2 


The function $L_h(y_i,g(_i))= (0,1-y_i g(_i))$ is called the .




[t]Zero-One Loss


8pt
Both the logistic loss and the hinge loss are convex upper bounds on the zero-one loss:

$$L_01(y_i,g(_i)) = [y_i (g(_i))]$$

The average zero-one loss over a data set is exactly the classification error rate.

This is the loss function we'd like to minimize, but this generally isn't computationally feasible, thus the need for surrogate loss functions.

Hinge loss has some advantages over logistic loss, as we'll see.






[t]Maximum Margin Property

Part of popularity of SVMs stems from the fact that the hinge loss results in the  decision boundary when the training cases are linearly separable.

2in
[width=2in]../Figures/svm-boundaries.png
2in
[width=2in]../Figures/svm-opt.png




[t]Support Vector Property

In the linearly separable case, some data points will always fall exactly on the margins. These points are called .

[width=2.5in]../Figures/svm-support-vectors.png



[t]SVMs vs Logistic Regression


8pt
SVMs and Logistic regression are both discriminative linear classifiers with identical capacity and space complexity. 

SVMs and Logistic regression have very similar convex loss functions and identical regularizers.

The hinge loss is not differentiale, unlike the logistic loss, so SVMs require more advanced optimization methods (sub-gradient descent or quadratic programming), but there are extremely good algorithms and implementations available.

The maximum-margin property of SVMs can yield better generalization than logistic regression when data is scarce.




[t]SVMs vs Logistic Regression


8pt

It is somewhat more difficult to form a multi-class SVM than a multi-class logistic regression model, and many implementations use ``hacks'' like one-vs-all.

Unlike logistic regression, SVMs do not produce probabilistic outputs, but again, there are hacks that can estimate probabilities.








[t]The Problem with Linear Models


8pt
The problem with linear classifiers is that their decision boundaries are by definition linear in the input feature space.

This means that they can have very high bias on complex data.

 How can we relax the constraint of linear decision boundaries while retaining the nice properties of linear classifiers?




[t]Basis Function Expansion


8pt
One very simple solution is to apply a set of functions $_1$,...,$_K$ to the
raw feature vector $$ to map it in to a new feature space:

$$() = [_1(),...,_K()]$$

This is called a  since $K>D$ in general. This requires that we know the functions $_1$,...,$_K$ that we want to apply in advance.

We then define a linear classifier (SVM or Logistic Regression) in this new feature space:

$$^T() + b$$




[t]Basis Function Expansion Examples


8pt
 We include all single features $x_d$, their squares $x_d^2$, and all products of two distinct features $x_dx_d'$.

 We include all single features $x_d$, and all unique products of between $2$ and $B$ features.

The problem is that the space complexity of representing the expanded set of features is essentially $O(D^B)$.

Next we'll see how this problem can be solved.







[t]Representer Theorem


8pt
One of the interesting properties of SVMs is that the optimal weight vectors can always be expressed as a weighted linear combination of the data vectors:

$$ = _j=1^N _j _j$$

This result is called the . 




[t]Dependence on Inner Products


8pt
Plugging this result back in to the SVM objective we find that the objective only depends on the data through inner products: $_j^T_i$:


g(_i) &= ^T_i + b 
= _j=1^N _j _j^T_i +b\\
||||_2^2 &= ^T 
= _j=1^N _i=1^N _j_i _j^T_i






[t]Basis Expansion and Representer Theorem


8pt
Under an arbitrary basis expansion $()$ this result becomes:


g((_i)) &= _j=1^N _j (_j)^T(_i) +b\\
||||_2^2  &= _j=1^N _i=1^N _j_i (_j)^T(_i) \\


It can be shown in the linearly separable case that the $_i$ parameters
for data cases that are not support vectors are always $0$.




[t]The Kernel Trick


8pt
Amazingly, for many useful basis function expansions $()$, it is possible to find a function $(,') =()^T(')$ that can compute the inner product under the basis expansion !

Such functions are called  and this is known as the ``Kernel Trick''.

Importantly, you can also directly learn the parameters $_i$ and $b$ using the kernel trick, without constructing the basis expansion.

Interestingly, there exist kernels for which the basis function expansion implied by the kernel isn't even finite dimensional!





[t]Examples of Kernel Functions


8pt
 $_P(,') = (^T' + 1)^B$

 $_G(,') = (-||-'||_2^2)$

Many more domain-specific kernels for strings, histograms, probability distributions, and other complex structured objects.





[t]Trade-Offs: Basis Expansion and Kernels


8pt
Linear classifiers are fast and space efficient, but can have high bias.

Basis expansion requires more space ($O(NK)$ for data and $O(K)$ for parameters), but yields non-linear classifiers that have lower bias.

Kernel SVMs actually require $O(N^2)$ space for storing all the kernel values during training and have $O(N)$ parameters. This can still be much lower than $O(NK)$ for large sets of basis functions. Kernels also yield non-linear classifiers that have lower bias.

Kernel SVMs often have at least two parameters ($C$ and a kernel hyperparameter). These need to be set jointly, which can be computationally expensive. 




[t]Trade-Offs: Basis Expansion and Kernels


8pt

Gaussian kernel SVMs also have infinite capacity, and a very closely related to weighted KNN.

Importantly, everything we said about SVMs and kernels is also true for logistic regression. Applying the kernel trick to logistic regression yields a model called ``kernel logistic regression'' or KLR.

KLR can exploit infinite dimensional feature spaces, can be learned with smooth optimization methods, supports probabilistic outputs, and has an easy multi-class generalization, but lacks the margin maximization property.